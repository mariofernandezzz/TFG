{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0497026",
   "metadata": {},
   "source": [
    "## Importación de librerías \n",
    "\n",
    "En esta sección se importan las principales librerías utilizadas en el preprocesamiento del texto.\n",
    "- **spaCy**: biblioteca de procesamiento del lenguaje natural (NLP) que permite realizar tareas como tokenización, eliminación de stopwords o lematización.\n",
    "    - Se incluye además la configuración personalizada del tokenizador de spaCy, que permite ajustar cómo se separan los tokens (por ejemplo, evitar divisiones innecesarias en palabras con guiones, puntos, etc.).\n",
    "- **pandas**: biblioteca para la manipulación de datos estructurados (tablas) de tipo DataFrame.\n",
    "- **collections.Counter**: conteo eficiente de elementos en colecciones.\n",
    "- **sklearn.preprocessing.normalize**: normalización de vectores o matrices.\n",
    "- **sklearn.metrics.pairwise.cosine_similarity**: cálculo de similitud coseno entre vectores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6e09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e1ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy: 3.8.4\n",
      "pandas: 2.2.3\n",
      "scikit-learn: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Librería que contiene herramientas de preprocesamiento\n",
    "import spacy\n",
    "\n",
    "# Herramientas internas de spaCy para modificar el tokenizador por defecto\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# Librería para carga de datos\n",
    "import pandas as pd\n",
    "\n",
    "# Librería para contar frecuencias\n",
    "from collections import Counter\n",
    "\n",
    "# Librerías para normalizar y aplicar similitud del coseno\n",
    "import sklearn\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bed08",
   "metadata": {},
   "source": [
    "## Definición de funciones y configuración de preprocesamiento\n",
    "\n",
    "Este bloque incluye todo lo necesario para preparar el texto antes de su análisis. Se utiliza el modelo `en_core_web_lg` de spaCy y se personaliza el tokenizador para evitar divisiones en guiones.\n",
    "\n",
    "### Componentes definidos:\n",
    "\n",
    "- **Carga y personalización del modelo spaCy**: se modifica el tokenizador para no dividir palabras por guiones.\n",
    "- **`tokenizar_texto_spacy(texto)`**: convierte una cadena de texto a minúsculas y la transforma en una lista de tokens.\n",
    "- **`eliminar_stopwords_spacy(tokens)`**: elimina tokens irrelevantes, incluyendo stopwords, palabras no alfabéticas y una lista personalizada de palabras excluidas.\n",
    "- **`lematizar_texto_spacy(tokens)`**: transforma cada token en su lema o raíz.\n",
    "- **`aplicar_mapeo_df(serie_de_listas, mapeo)`**: aplica un diccionario de equivalencias léxicas para unificar términos similares.\n",
    "- **`palabras_extra`**: lista manual de palabras irrelevantes específicas del contexto del corpus.\n",
    "- **`mapeo`**: diccionario de normalización construido a partir de un archivo `.csv` externo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1569af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo grande de inglés (incluye vectores de palabras)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Eliminar los guiones como separadores internos para evitar fragmentación de tokens como \"ai-based\" en \"ai\" \"based\"\n",
    "infixes = [x for x in nlp.Defaults.infixes if \"-\" not in x]\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "# Reconfigurar el tokenizador con las nuevas reglas de infijos\n",
    "nlp.tokenizer = Tokenizer(\n",
    "    nlp.vocab,\n",
    "    rules=nlp.Defaults.tokenizer_exceptions,\n",
    "    prefix_search=nlp.tokenizer.prefix_search,\n",
    "    suffix_search=nlp.tokenizer.suffix_search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=nlp.tokenizer.token_match\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Función de tokenización básica con spaCy en la que también se pasa el texto en minúsculas\n",
    "# La separación en tokens se realiza según las reglas del modelo de lenguaje cargado,\n",
    "# teniendo en cuenta espacios, puntuación, signos de puntuación interna (como guiones, apóstrofes, etc.)\n",
    "# y excepciones predefinidas. En este caso, se ha modificado el tokenizer para que no divida por guiones.\n",
    "def tokenizar_texto_spacy(texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    return list(doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lista personalizada de palabras a ignorar (además de las stopwords de spaCy)\n",
    "palabras_extra = [\n",
    "    \"co\", \"digital\", \"alexa\", \"anytime\", \"cortana\", \"arduino\", \"bitcoin\",\n",
    "    \"deepl\", \"digitally\", \"digitise\", \"dropbox\", \"duckduckgo\", \"e\", \"etc\",\n",
    "    \"eurostat\", \"example\", \"facebook\", \"google\", \"miro\", \"t\", \"twitter\",\n",
    "    \"tpm\", \"youtube\", \"vs\", \"whatsapp\", \"wikipedia\", \"x\", \"en\", \"se\", \"lego\",\n",
    "    \"non\", \"python\", \"ros\", \"scratch\", \"siri\", \"-\", \"non-digital\", \"one-time\",\n",
    "    \"bas\", \"digital-based\", \"digitise\", \"digitised\", \"digitization\", \"s\", \"whilst\"\n",
    "]\n",
    "\n",
    "# Asegurar que \"3d\" y \"3-d\" no se consideren stopwords, ya que el eliminador de stopwrods \n",
    "# elimina todas con números\n",
    "nlp.vocab[\"3d\"].is_alpha = True\n",
    "nlp.vocab[\"3-d\"].is_alpha = True\n",
    "nlp.vocab[\"2d\"].is_alpha = True\n",
    "nlp.vocab[\"2-d\"].is_alpha = True\n",
    "\n",
    "# Función para eliminar stopwords y tokens no alfabéticos.\n",
    "# Ignora las palabras_extra\n",
    "def eliminar_stopwords_spacy(tokens):\n",
    "    return [\n",
    "        token for token in tokens\n",
    "        if not token.is_stop\n",
    "        and token.is_alpha\n",
    "        and token.text.lower() not in palabras_extra\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Función de lematización del modelos cargado\n",
    "def lematizar_texto_spacy(tokens):\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "#carga del archivo que contiene los mapeos correspondientes y conversión a tipo DataFrame para ser tratado como tabla\n",
    "ruta_mapeo = \"../utils/mapping.csv\"\n",
    "df_mapeo = pd.read_csv(ruta_mapeo)\n",
    "\n",
    "# Construcción del diccionario: original -> equivalente\n",
    "mapeo = dict(zip(df_mapeo[\"original\"], df_mapeo[\"equivalente\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2dfe4",
   "metadata": {},
   "source": [
    "## Función `comparar_texto_con_frecuencias`\n",
    "\n",
    "Compara un texto con grupos de una tabla de frecuencias y calcula la similitud basada en palabras comunes.\n",
    "\n",
    "**Parámetros:**\n",
    "\n",
    "- `texto`: lista de palabras del texto preprocesado.\n",
    "- `df_pivot`: DataFrame con palabras como filas y grupos como columnas, con frecuencias.\n",
    "- `top_n`: número de palabras clave a mostrar por grupo (por defecto 5).\n",
    "\n",
    "**Funcionamiento resumido:**\n",
    "\n",
    "1. Obtiene el vocabulario del DataFrame.\n",
    "2. Crea un vector con las frecuencias de las palabras del texto en el vocabulario.\n",
    "3. Combina este vector con los vectores de frecuencias de los grupos.\n",
    "4. Normaliza los vectores para calcular similitud coseno.\n",
    "5. Calcula la similitud entre el texto y cada grupo.\n",
    "6. Identifica las palabras que más contribuyen a la similitud para cada grupo.\n",
    "7. Devuelve un DataFrame con grupo, similitud y palabras clave ordenado por similitud.\n",
    "\n",
    "**Devuelve:**\n",
    "\n",
    "Un DataFrame con columnas: `'Grupo'`, `'Similitud'` y `'Palabras_clave'`.\n",
    "\n",
    "## Función `aplicar_mapeo_a_texto(lista_tokens, mapeo)`\n",
    "\n",
    "Reemplaza cada token en una lista de estos según un diccionario de mapeo léxico para unificar términos similares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06da384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_texto_con_frecuencias(texto, df_pivot, top_n=5):\n",
    "\n",
    "    # 1. Vocabulario (palabras que aparecen en df_pivot)\n",
    "    vocabulario = df_pivot.index.tolist()\n",
    "\n",
    "    # 2. Vector del texto: contar solo palabras del vocabulario\n",
    "    conteo_texto = Counter(p for p in texto if p in vocabulario)\n",
    "    vector_texto = pd.Series([conteo_texto.get(p, 0) for p in vocabulario], index=vocabulario).to_frame(name=\"Texto\")\n",
    "\n",
    "    # 3. Concatenar vectores del texto y de grupos\n",
    "    matriz_completa = pd.concat([df_pivot, vector_texto], axis=1).fillna(0)\n",
    "\n",
    "    # 4. Normalizar vectores para cálculo coseno\n",
    "    matriz_normalizada = normalize(matriz_completa.T)\n",
    "\n",
    "    # 5. Calcular similitud coseno entre texto (última fila) y cada grupo (todas menos última)\n",
    "    similitudes = cosine_similarity([matriz_normalizada[-1]], matriz_normalizada[:-1])[0]\n",
    "\n",
    "    # 6. Preparar resultados básicos\n",
    "    nombres_grupo = df_pivot.columns.tolist()\n",
    "    resultados = pd.DataFrame({\n",
    "        'Grupo': [g if isinstance(g, str) else ' - '.join(map(str, g)) for g in nombres_grupo],\n",
    "        'Similitud': similitudes\n",
    "    })\n",
    "\n",
    "    # 7. Calcular contribuciones por palabra a la similitud de cada grupo\n",
    "    #    La contribución para cada palabra = frecuencia_texto * frecuencia_grupo\n",
    "    #    Ordenamos y extraemos las top_n palabras para cada grupo\n",
    "    palabras_clave_por_grupo = []\n",
    "    for grupo in nombres_grupo:\n",
    "        contribuciones = []\n",
    "        for palabra in vocabulario:\n",
    "            f_texto = vector_texto.at[palabra, \"Texto\"]\n",
    "            f_grupo = df_pivot.at[palabra, grupo]\n",
    "            if f_texto > 0 and f_grupo > 0:\n",
    "                contribuciones.append((palabra, f_texto * f_grupo))\n",
    "\n",
    "        # Ordenar por contribución descendente\n",
    "        contribuciones.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_palabras = [p for p, _ in contribuciones[:top_n]]\n",
    "        palabras_clave_por_grupo.append(\", \".join(top_palabras))\n",
    "\n",
    "    resultados[\"Palabras_clave\"] = palabras_clave_por_grupo\n",
    "\n",
    "    # 8. Ordenar resultados por similitud descendente\n",
    "    resultados = resultados.sort_values(by=\"Similitud\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return resultados\n",
    "\n",
    "def aplicar_mapeo_a_texto(lista_tokens, mapeo):\n",
    "    return [mapeo.get(token, token) for token in lista_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df0697",
   "metadata": {},
   "source": [
    "## Bloque para modificar el texto a analizar\n",
    "\n",
    "En este bloque el usuario debe introducir el texto que desea analizar. El texto pasará por varias etapas de preprocesamiento: tokenización, eliminación de stopwords, lematización y aplicación del mapeo léxico definido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bc4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXTO A COMPARAR\n",
    "texto = \"In today digital society, individuals need to use various tools and applications to access, manage, and evaluate information efficiently. Being able to find trustworthy sources, use appropriate technologies, and apply knowledge critically is essential. People should also understand how digital tools influence behavior, communication, and decision-making. The ability to use AI systems and understand data privacy is increasingly important in both education and work contexts.\"\n",
    "texto1 = tokenizar_texto_spacy(texto)\n",
    "texto2 = eliminar_stopwords_spacy(texto1)\n",
    "texto3 = lematizar_texto_spacy(texto2)\n",
    "texto4 = aplicar_mapeo_a_texto(texto3, mapeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0afb8",
   "metadata": {},
   "source": [
    "## Comparación de texto seleccionado con texto de DigComp\n",
    "\n",
    "### Dimensión 2 agrupado por áreas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157f44e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Grupo  Similitud                                   Palabras_clave\n",
      "0     2   0.543914         use, knowledge, tool, communicate, datum\n",
      "1     5   0.434283     use, tool, knowledge, technology, understand\n",
      "2     4   0.357486  use, datum, knowledge, appropriate, application\n",
      "3     1   0.327865       datum, use, information, knowledge, access\n",
      "4     3   0.250024              use, knowledge, datum, access, find\n"
     ]
    }
   ],
   "source": [
    "ruta_d2_areas = \"../results/digcomp/frecuencias_d2_areas.csv\"\n",
    "frecuencias_2_areas = pd.read_csv(ruta_d2_areas, index_col=0)\n",
    "\n",
    "resultados_d2_areas = comparar_texto_con_frecuencias(texto4, frecuencias_2_areas, top_n=5)\n",
    "print(resultados_d2_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8373b",
   "metadata": {},
   "source": [
    "### Dimensión 2 agrupado por competencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851ad973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Grupo  Similitud                                     Palabras_clave\n",
      "0    2.4   0.471332            use, tool, knowledge, work, appropriate\n",
      "1    5.4   0.435710              use, tool, understand, ai, technology\n",
      "2    5.2   0.374324                 use, tool, knowledge, need, access\n",
      "3    5.3   0.350115      use, tool, knowledge, technology, information\n",
      "4    2.1   0.349856     use, communicate, tool, knowledge, appropriate\n",
      "5    2.3   0.298903                use, ai, knowledge, society, system\n",
      "6    1.2   0.277407          information, source, datum, knowledge, ai\n",
      "7    4.3   0.275510     use, ability, behaviour, knowledge, technology\n",
      "8    1.3   0.258482                 datum, use, application, knowledge\n",
      "9    2.6   0.247076              datum, use, knowledge, manage, system\n",
      "10   4.4   0.225460                  use, knowledge, datum, technology\n",
      "11   1.1   0.224129         use, information, access, datum, knowledge\n",
      "12   2.5   0.213158  behaviour, communicate, people, knowledge, manage\n",
      "13   3.2   0.208463                                 use, knowledge, ai\n",
      "14   3.1   0.201206                             use, access, knowledge\n",
      "15   2.2   0.199053                     use, knowledge, system, source\n",
      "16   4.2   0.187535             datum, appropriate, knowledge, privacy\n",
      "17   3.3   0.180368                                     use, knowledge\n",
      "18   4.1   0.149288                           knowledge, datum, access\n",
      "19   3.4   0.099409                                   knowledge, datum\n",
      "20   5.1   0.065357                                          knowledge\n"
     ]
    }
   ],
   "source": [
    "ruta_d2_competencias = \"../results/digcomp/frecuencias_d2_competencia.csv\"\n",
    "frecuencias_2_competencias = pd.read_csv(ruta_d2_competencias, index_col=0)\n",
    "\n",
    "resultados_d2_competencias = comparar_texto_con_frecuencias(texto4, frecuencias_2_competencias, top_n=5)\n",
    "print(resultados_d2_competencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0812f5",
   "metadata": {},
   "source": [
    "### Dimensión 3 agrupado por competencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847829d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Grupo  Similitud                                 Palabras_clave\n",
      "0    4.4   0.350339                                use, technology\n",
      "1    2.4   0.310835                               tool, technology\n",
      "2    1.2   0.299945         datum, information, evaluation, source\n",
      "3    5.2   0.279389                         need, tool, technology\n",
      "4    2.1   0.278423  technology, communicate, appropriate, context\n",
      "5    5.3   0.253024                    tool, technology, knowledge\n",
      "6    2.3   0.241227               society, technology, appropriate\n",
      "7    3.3   0.220755                      apply, information, datum\n",
      "8    2.2   0.212664           technology, information, appropriate\n",
      "9    1.1   0.205181                             information, datum\n",
      "10   4.2   0.167248                                 datum, privacy\n",
      "11   2.6   0.159111                                    tool, datum\n",
      "12   1.3   0.134249                             datum, information\n",
      "13   4.3   0.092547                                     technology\n",
      "14   3.4   0.080397                                         system\n",
      "15   5.4   0.057378                                           need\n",
      "16   3.2   0.049846                                    information\n",
      "17   4.1   0.049383                                        privacy\n",
      "18   2.5   0.000000                                               \n",
      "19   3.1   0.000000                                               \n",
      "20   5.1   0.000000                                               \n"
     ]
    }
   ],
   "source": [
    "ruta_d3_competencias = \"../results/digcomp/frecuencias_d3_competencia.csv\"\n",
    "frecuencias_3_competencias = pd.read_csv(ruta_d3_competencias, index_col=0)\n",
    "\n",
    "resultados_d3_competencias = comparar_texto_con_frecuencias(texto4, frecuencias_3_competencias, top_n=5)\n",
    "print(resultados_d3_competencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60db9a",
   "metadata": {},
   "source": [
    "### Dimensión 3 agrupado por nivel de competencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350584d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Grupo  Similitud                              Palabras_clave\n",
      "0            Advanced   0.496906  appropriate, use, datum, technology, apply\n",
      "1        Intermediate   0.235273        information, technology, datum, need\n",
      "2          Foundation   0.131585              technology, datum, information\n",
      "3  Highly specialised   0.127935                       knowledge, technology\n"
     ]
    }
   ],
   "source": [
    "ruta_d3_tipoNivel = \"../results/digcomp/frecuencias_d3_tipoNivel.csv\"\n",
    "frecuencias_3_tipoNivel = pd.read_csv(ruta_d3_tipoNivel, index_col=0)\n",
    "\n",
    "resultados_d3_tipoNivel = comparar_texto_con_frecuencias(texto4, frecuencias_3_tipoNivel, top_n=5)\n",
    "print(resultados_d3_tipoNivel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b82f08",
   "metadata": {},
   "source": [
    "### Dimensión 3 agrupado por subnivel de competencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22265a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Grupo  Similitud                               Palabras_clave\n",
      "0     5   0.375293   apply, use, datum, information, technology\n",
      "1     6   0.352262  appropriate, technology, datum, information\n",
      "2     4   0.285143         information, technology, datum, need\n",
      "3     7   0.169703                        knowledge, technology\n",
      "4     3   0.141208               technology, information, datum\n",
      "5     2   0.131585               technology, datum, information\n",
      "6     1   0.131585               technology, datum, information\n",
      "7     8   0.000000                                             \n"
     ]
    }
   ],
   "source": [
    "ruta_d3_subnivel = \"../results/digcomp/frecuencias_d3_subnivel.csv\"\n",
    "frecuencias_3_subnivel = pd.read_csv(ruta_d3_subnivel, index_col=0)\n",
    "\n",
    "resultados_d3_subnivel = comparar_texto_con_frecuencias(texto4, frecuencias_3_subnivel, top_n=5)\n",
    "print(resultados_d3_subnivel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce8659",
   "metadata": {},
   "source": [
    "### Dimensión 4 agrupado por competencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53884ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Grupo  Similitud                                 Palabras_clave\n",
      "0    4.4   0.350339                                use, technology\n",
      "1    2.4   0.310835                               tool, technology\n",
      "2    1.2   0.299945         datum, information, evaluation, source\n",
      "3    5.2   0.279389                         need, tool, technology\n",
      "4    2.1   0.278423  technology, communicate, appropriate, context\n",
      "5    5.3   0.253024                    tool, technology, knowledge\n",
      "6    2.3   0.241227               society, technology, appropriate\n",
      "7    3.3   0.220755                      apply, information, datum\n",
      "8    2.2   0.212664           technology, information, appropriate\n",
      "9    1.1   0.205181                             information, datum\n",
      "10   4.2   0.167248                                 datum, privacy\n",
      "11   2.6   0.159111                                    tool, datum\n",
      "12   1.3   0.134249                             datum, information\n",
      "13   4.3   0.092547                                     technology\n",
      "14   3.4   0.080397                                         system\n",
      "15   5.4   0.057378                                           need\n",
      "16   3.2   0.049846                                    information\n",
      "17   4.1   0.049383                                        privacy\n",
      "18   2.5   0.000000                                               \n",
      "19   3.1   0.000000                                               \n",
      "20   5.1   0.000000                                               \n"
     ]
    }
   ],
   "source": [
    "ruta_d4_competencias = \"../results/digcomp/frecuencias_d4_competencias.csv\"\n",
    "frecuencias_4_competencias = pd.read_csv(ruta_d3_competencias, index_col=0)\n",
    "\n",
    "resultados_d4_competencias = comparar_texto_con_frecuencias(texto4, frecuencias_4_competencias, top_n=5)\n",
    "print(resultados_d4_competencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1048f1",
   "metadata": {},
   "source": [
    "### Dimensión 4 agrupado por tipo de ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b67df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Grupo  Similitud                           Palabras_clave\n",
      "0     Skills   0.533470     use, knowledge, tool, ability, datum\n",
      "1  Knowledge   0.419379          use, knowledge, datum, tool, ai\n",
      "2  Attitudes   0.411161  use, information, ai, technology, datum\n"
     ]
    }
   ],
   "source": [
    "ruta_d4_tipoEjemplo = \"../results/digcomp/frecuencias_d4_tipoEjemplo.csv\"\n",
    "frecuencias_4_tipoEjemplo = pd.read_csv(ruta_d4_tipoEjemplo, index_col=0)\n",
    "\n",
    "resultados_d4_tipoEjemplo = comparar_texto_con_frecuencias(texto4, frecuencias_4_tipoEjemplo, top_n=5)\n",
    "print(resultados_d4_tipoEjemplo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
