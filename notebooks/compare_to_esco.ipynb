{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd7769b",
   "metadata": {},
   "source": [
    "## Importación de librerías \n",
    "\n",
    "En esta sección se importan las principales librerías utilizadas en el preprocesamiento del texto.\n",
    "- **spaCy**: biblioteca de procesamiento del lenguaje natural (NLP) que permite realizar tareas como tokenización, eliminación de stopwords o lematización.\n",
    "    - Se incluye además la configuración personalizada del tokenizador de spaCy, que permite ajustar cómo se separan los tokens (por ejemplo, evitar divisiones innecesarias en palabras con guiones, puntos, etc.).\n",
    "- **pandas**: biblioteca para la manipulación de datos estructurados (tablas) de tipo DataFrame.\n",
    "- **collections.Counter**: conteo eficiente de elementos en colecciones.\n",
    "- **sklearn.preprocessing.normalize**: normalización de vectores o matrices.\n",
    "- **sklearn.metrics.pairwise.cosine_similarity**: cálculo de similitud coseno entre vectores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5240a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librería que contiene herramientas de preprocesamiento\n",
    "import spacy\n",
    "\n",
    "# Herramientas internas de spaCy para modificar el tokenizador por defecto\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# Librería para carga de datos\n",
    "import pandas as pd\n",
    "\n",
    "# Librería para contar frecuencias\n",
    "from collections import Counter\n",
    "\n",
    "# Librerías para normalizar y aplicar similitud del coseno\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b09d8b",
   "metadata": {},
   "source": [
    "## Definición de funciones y configuración de preprocesamiento\n",
    "\n",
    "Este bloque incluye todo lo necesario para preparar el texto antes de su análisis. Se utiliza el modelo `en_core_web_lg` de spaCy y se personaliza el tokenizador para evitar divisiones en guiones.\n",
    "\n",
    "### Componentes definidos:\n",
    "\n",
    "- **Carga y personalización del modelo spaCy**: se modifica el tokenizador para no dividir palabras por guiones.\n",
    "- **`tokenizar_texto_spacy(texto)`**: convierte una cadena de texto a minúsculas y la transforma en una lista de tokens.\n",
    "- **`eliminar_stopwords_spacy(tokens)`**: elimina tokens irrelevantes, incluyendo stopwords, palabras no alfabéticas y una lista personalizada de palabras excluidas.\n",
    "- **`lematizar_texto_spacy(tokens)`**: transforma cada token en su lema o raíz.\n",
    "- **`aplicar_mapeo_df(serie_de_listas, mapeo)`**: aplica un diccionario de equivalencias léxicas para unificar términos similares.\n",
    "- **`palabras_extra`**: lista manual de palabras irrelevantes específicas del contexto del corpus.\n",
    "- **`mapeo`**: diccionario de normalización construido a partir de un archivo `.csv` externo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a9d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo grande de inglés (incluye vectores de palabras)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Eliminar los guiones como separadores internos para evitar fragmentación de tokens como \"ai-based\" en \"ai\" \"based\"\n",
    "infixes = [x for x in nlp.Defaults.infixes if \"-\" not in x]\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "# Reconfigurar el tokenizador con las nuevas reglas de infijos\n",
    "nlp.tokenizer = Tokenizer(\n",
    "    nlp.vocab,\n",
    "    rules=nlp.Defaults.tokenizer_exceptions,\n",
    "    prefix_search=nlp.tokenizer.prefix_search,\n",
    "    suffix_search=nlp.tokenizer.suffix_search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=nlp.tokenizer.token_match\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Función de tokenización básica con spaCy en la que también se pasa el texto en minúsculas\n",
    "# La separación en tokens se realiza según las reglas del modelo de lenguaje cargado,\n",
    "# teniendo en cuenta espacios, puntuación, signos de puntuación interna (como guiones, apóstrofes, etc.)\n",
    "# y excepciones predefinidas. En este caso, se ha modificado el tokenizer para que no divida por guiones.\n",
    "def tokenizar_texto_spacy(texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    return list(doc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lista personalizada de palabras a ignorar (además de las stopwords de spaCy)\n",
    "palabras_extra = [\n",
    "    \"co\", \"digital\", \"alexa\", \"anytime\", \"cortana\", \"arduino\", \"bitcoin\",\n",
    "    \"deepl\", \"digitally\", \"digitise\", \"dropbox\", \"duckduckgo\", \"e\", \"etc\",\n",
    "    \"eurostat\", \"example\", \"facebook\", \"google\", \"miro\", \"t\", \"twitter\",\n",
    "    \"tpm\", \"youtube\", \"vs\", \"whatsapp\", \"wikipedia\", \"x\", \"en\", \"se\", \"lego\",\n",
    "    \"non\", \"python\", \"ros\", \"scratch\", \"siri\", \"-\", \"non-digital\", \"one-time\",\n",
    "    \"bas\", \"digital-based\", \"digitise\", \"digitised\", \"digitization\", \"s\", \"whilst\"\n",
    "]\n",
    "\n",
    "# Asegurar que \"3d\" y \"3-d\" no se consideren stopwords, ya que el eliminador de stopwrods \n",
    "# elimina todas con números\n",
    "nlp.vocab[\"3d\"].is_alpha = True\n",
    "nlp.vocab[\"3-d\"].is_alpha = True\n",
    "nlp.vocab[\"2d\"].is_alpha = True\n",
    "nlp.vocab[\"2-d\"].is_alpha = True\n",
    "\n",
    "# Función para eliminar stopwords y tokens no alfabéticos.\n",
    "# Ignora las palabras_extra\n",
    "def eliminar_stopwords_spacy(tokens):\n",
    "    return [\n",
    "        token for token in tokens\n",
    "        if not token.is_stop\n",
    "        and token.is_alpha\n",
    "        and token.text.lower() not in palabras_extra\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Función de lematización del modelos cargado\n",
    "def lematizar_texto_spacy(tokens):\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "# Función de mapeo de palabras en función de un diccionario de mapeo para normalización de términos\n",
    "def aplicar_mapeo(serie_de_listas, mapeo):\n",
    "    return serie_de_listas.apply(lambda tokens: [mapeo.get(token, token) for token in tokens])\n",
    "\n",
    "\n",
    "\n",
    "#carga del archivo que contiene los mapeos correspondientes y conversión a tipo DataFrame para ser tratado como tabla\n",
    "ruta_mapeo = \"../utils/mapping.csv\"\n",
    "df_mapeo = pd.read_csv(ruta_mapeo)\n",
    "\n",
    "# Construcción del diccionario: original -> equivalente\n",
    "mapeo = dict(zip(df_mapeo[\"original\"], df_mapeo[\"equivalente\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e91e3d",
   "metadata": {},
   "source": [
    "## Función `comparar_texto_con_frecuencias`\n",
    "\n",
    "Compara un texto con grupos de una tabla de frecuencias y calcula la similitud basada en palabras comunes.\n",
    "\n",
    "**Parámetros:**\n",
    "\n",
    "- `texto`: lista de palabras del texto preprocesado.\n",
    "- `df_pivot`: DataFrame con palabras como filas y grupos como columnas, con frecuencias.\n",
    "- `top_n`: número de palabras clave a mostrar por grupo (por defecto 5).\n",
    "\n",
    "**Funcionamiento resumido:**\n",
    "\n",
    "1. Obtiene el vocabulario del DataFrame.\n",
    "2. Crea un vector con las frecuencias de las palabras del texto en el vocabulario.\n",
    "3. Combina este vector con los vectores de frecuencias de los grupos.\n",
    "4. Normaliza los vectores para calcular similitud coseno.\n",
    "5. Calcula la similitud entre el texto y cada grupo.\n",
    "6. Identifica las palabras que más contribuyen a la similitud para cada grupo.\n",
    "7. Devuelve un DataFrame con grupo, similitud y palabras clave ordenado por similitud.\n",
    "\n",
    "**Devuelve:**\n",
    "\n",
    "Un DataFrame con columnas: `'Grupo'`, `'Similitud'` y `'Palabras_clave'`.\n",
    "\n",
    "## Función `aplicar_mapeo_a_texto(lista_tokens, mapeo)`\n",
    "\n",
    "Reemplaza cada token en una lista de estos según un diccionario de mapeo léxico para unificar términos similares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fed8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_texto_con_frecuencias(texto, df_pivot, top_n=5):\n",
    "\n",
    "    # 1. Vocabulario (palabras que aparecen en df_pivot)\n",
    "    vocabulario = df_pivot.index.tolist()\n",
    "\n",
    "    # 2. Vector del texto: contar solo palabras del vocabulario\n",
    "    conteo_texto = Counter(p for p in texto if p in vocabulario)\n",
    "    vector_texto = pd.Series([conteo_texto.get(p, 0) for p in vocabulario], index=vocabulario).to_frame(name=\"Texto\")\n",
    "\n",
    "    # 3. Concatenar vectores del texto y de grupos\n",
    "    matriz_completa = pd.concat([df_pivot, vector_texto], axis=1).fillna(0)\n",
    "\n",
    "    # 4. Normalizar vectores para cálculo coseno\n",
    "    matriz_normalizada = normalize(matriz_completa.T)\n",
    "\n",
    "    # 5. Calcular similitud coseno entre texto (última fila) y cada grupo (todas menos última)\n",
    "    similitudes = cosine_similarity([matriz_normalizada[-1]], matriz_normalizada[:-1])[0]\n",
    "\n",
    "    # 6. Preparar resultados básicos\n",
    "    nombres_grupo = df_pivot.columns.tolist()\n",
    "    resultados = pd.DataFrame({\n",
    "        'Grupo': [g if isinstance(g, str) else ' - '.join(map(str, g)) for g in nombres_grupo],\n",
    "        'Similitud': similitudes\n",
    "    })\n",
    "\n",
    "    # 7. Calcular contribuciones por palabra a la similitud de cada grupo\n",
    "    #    La contribución para cada palabra = frecuencia_texto * frecuencia_grupo\n",
    "    #    Ordenamos y extraemos las top_n palabras para cada grupo\n",
    "    palabras_clave_por_grupo = []\n",
    "    for grupo in nombres_grupo:\n",
    "        contribuciones = []\n",
    "        for palabra in vocabulario:\n",
    "            f_texto = vector_texto.at[palabra, \"Texto\"]\n",
    "            f_grupo = df_pivot.at[palabra, grupo]\n",
    "            if f_texto > 0 and f_grupo > 0:\n",
    "                contribuciones.append((palabra, f_texto * f_grupo))\n",
    "\n",
    "        # Ordenar por contribución descendente\n",
    "        contribuciones.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_palabras = [p for p, _ in contribuciones[:top_n]]\n",
    "        palabras_clave_por_grupo.append(\", \".join(top_palabras))\n",
    "\n",
    "    resultados[\"Palabras_clave\"] = palabras_clave_por_grupo\n",
    "\n",
    "    # 8. Ordenar resultados por similitud descendente\n",
    "    resultados = resultados.sort_values(by=\"Similitud\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return resultados\n",
    "\n",
    "def aplicar_mapeo_a_texto(lista_tokens, mapeo):\n",
    "    return [mapeo.get(token, token) for token in lista_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d139e",
   "metadata": {},
   "source": [
    "## Bloque para modificar el texto a analizar\n",
    "\n",
    "En este bloque el usuario debe introducir el texto que desea analizar. El texto pasará por varias etapas de preprocesamiento: tokenización, eliminación de stopwords, lematización y aplicación del mapeo léxico definido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3a7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXTO A COMPARAR\n",
    "texto = \"In today digital society, individuals need to use various tools and applications to access, manage, and evaluate information efficiently. Being able to find trustworthy sources, use appropriate technologies, and apply knowledge critically is essential. People should also understand how digital tools influence behavior, communication, and decision-making. The ability to use AI systems and understand data privacy is increasingly important in both education and work contexts.\"\n",
    "texto1 = tokenizar_texto_spacy(texto)\n",
    "texto2 = eliminar_stopwords_spacy(texto1)\n",
    "texto3 = lematizar_texto_spacy(texto2)\n",
    "texto4 = aplicar_mapeo_a_texto(texto3, mapeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96040a6",
   "metadata": {},
   "source": [
    "## Comparación de texto seleccionado con texto de ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db060095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Grupo  Similitud  \\\n",
      "0    using digital tools for collaboration and prod...   0.571880   \n",
      "1                              performing calculations   0.493197   \n",
      "2              digital communication and collaboration   0.470422   \n",
      "3                                       use e-services   0.410365   \n",
      "4         managing, gathering and storing digital data   0.400857   \n",
      "..                                                 ...        ...   \n",
      "416                     use computer-aided translation   0.000000   \n",
      "417                            use learning strategies   0.000000   \n",
      "418                        use measurement instruments   0.000000   \n",
      "419                                       CAD software   0.000000   \n",
      "420   working with machinery and specialised equipment   0.000000   \n",
      "\n",
      "                              Palabras_clave  \n",
      "0      use, manage, tool, technology, system  \n",
      "1                                        use  \n",
      "2         use, tool, technology, appropriate  \n",
      "3                            use, technology  \n",
      "4    use, datum, manage, system, information  \n",
      "..                                       ...  \n",
      "416                                           \n",
      "417                                           \n",
      "418                                           \n",
      "419                                           \n",
      "420                                           \n",
      "\n",
      "[421 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ruta_frecuencias_esco = \"../results/esco/frecuencias_esco.csv\"\n",
    "frecuencias_esco = pd.read_csv(ruta_frecuencias_esco, index_col=0)\n",
    "\n",
    "resultados_esco = comparar_texto_con_frecuencias(texto4, frecuencias_esco, top_n=5)\n",
    "print(resultados_esco)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
